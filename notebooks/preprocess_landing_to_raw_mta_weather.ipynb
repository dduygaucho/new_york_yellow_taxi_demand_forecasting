{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gdp\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp,hour,\\\n",
    "    dayofmonth,date_format,desc\n",
    "\n",
    "def landing_to_raw(df, cols, time_col = None, boolean_weather_dataset = None):\n",
    "    \"\"\"\n",
    "    This function converts the df selected on given cols, and output them\n",
    "    to the raw layer\n",
    "\n",
    "    boolean_weather_dataset: boolean values to determine wheter mta or weather\n",
    "    time_col: the column related to time in dataframe\n",
    "    \"\"\"\n",
    "    # select columns \n",
    "    intermediate = df.select(*cols)\n",
    "\n",
    "    # consistent_col_casing\n",
    "    consistent_col_casing = \\\n",
    "        [F.col(col_name).alias(col_name.lower()) for col_name in intermediate.columns]\n",
    "    intermediate = intermediate.select(*consistent_col_casing)\n",
    "\n",
    "    # convert date data type: string --> timestamp\n",
    "    if boolean_weather_dataset:\n",
    "        # Y-M-D, H:m:s, 24 hour format --> weather dataset\n",
    "        intermediate = intermediate.withColumn\\\n",
    "            (time_col, to_timestamp(col(time_col), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "    else:\n",
    "        # M-D-Y, h:m:s a: 12 hour format --> mta dataset\n",
    "        intermediate = intermediate.withColumn\\\n",
    "            (time_col, to_timestamp(col(time_col), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "        # Parse the \"value\" column from string to integer\n",
    "        intermediate = \\\n",
    "            intermediate.withColumn(\"ridership\", col(\"ridership\").cast(\"int\"))\n",
    "    return intermediate\n",
    "\n",
    "\n",
    "def statistics(df):\n",
    "    \"\"\"\n",
    "    Print out the number of rows and columns of this dataframe\n",
    "    \"\"\"\n",
    "    # Count the number of rows (equivalent to shape[0] in Pandas)\n",
    "    num_rows = df.count()\n",
    "\n",
    "    # Get the list of column names (equivalent to columns in Pandas)\n",
    "    column_names = df.columns\n",
    "\n",
    "    print(f\"Number of rows: {num_rows}\")\n",
    "    print(f\"Number of columns: {len(column_names)}\")\n",
    "    print(\"Column names:\", column_names)\n",
    "\n",
    "\n",
    "def save_to_parquet(df, file_path):\n",
    "    \"\"\"\n",
    "    save the dataframe into parquet format in file_path\n",
    "    \"\"\"\n",
    "    df \\\n",
    "        .coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather dataset\n",
    "Obtained from Integrated Weather Surface (pulled on 08/08/2023)\n",
    "Note that when rerun the shape can be different as these links are still updated daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/21 03:25:20 WARN Utils: Your hostname, DESKTOP-LHMPQFC resolves to a loopback address: 127.0.1.1; using 172.19.194.216 instead (on interface eth0)\n",
      "23/08/21 03:25:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/21 03:25:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13344 100\n",
      "8475 95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>date</th><th>wnd</th><th>tmp</th></tr>\n",
       "<tr><td>2022-01-01 00:00:00</td><td>200,1,N,0031,1</td><td>+0094,1</td></tr>\n",
       "<tr><td>2022-01-01 00:51:00</td><td>200,5,N,0031,5</td><td>+0094,5</td></tr>\n",
       "<tr><td>2022-01-01 01:51:00</td><td>200,5,N,0031,5</td><td>+0089,5</td></tr>\n",
       "<tr><td>2022-01-01 02:41:00</td><td>200,5,N,0036,5</td><td>+0094,5</td></tr>\n",
       "<tr><td>2022-01-01 02:51:00</td><td>220,5,N,0026,5</td><td>+0094,5</td></tr>\n",
       "<tr><td>2022-01-01 03:00:00</td><td>220,1,N,0026,1</td><td>+0094,1</td></tr>\n",
       "<tr><td>2022-01-01 03:16:00</td><td>200,5,N,0015,5</td><td>+0089,5</td></tr>\n",
       "<tr><td>2022-01-01 03:51:00</td><td>190,5,N,0026,5</td><td>+0089,5</td></tr>\n",
       "<tr><td>2022-01-01 04:17:00</td><td>180,5,N,0026,5</td><td>+0094,5</td></tr>\n",
       "<tr><td>2022-01-01 04:51:00</td><td>170,5,N,0036,5</td><td>+0094,5</td></tr>\n",
       "<tr><td>2022-01-01 04:59:00</td><td>999,9,9,9999,9</td><td>+9999,9</td></tr>\n",
       "<tr><td>2022-01-01 04:59:00</td><td>999,9,9,9999,9</td><td>+9999,9</td></tr>\n",
       "<tr><td>2022-01-01 05:49:00</td><td>180,5,N,0041,5</td><td>+0090,5</td></tr>\n",
       "<tr><td>2022-01-01 05:51:00</td><td>180,5,N,0036,5</td><td>+0094,5</td></tr>\n",
       "<tr><td>2022-01-01 06:00:00</td><td>180,1,N,0036,1</td><td>+0094,1</td></tr>\n",
       "<tr><td>2022-01-01 06:18:00</td><td>180,5,N,0026,5</td><td>+0089,5</td></tr>\n",
       "<tr><td>2022-01-01 06:51:00</td><td>160,5,N,0021,5</td><td>+0094,5</td></tr>\n",
       "<tr><td>2022-01-01 07:09:00</td><td>130,5,N,0015,5</td><td>+0094,5</td></tr>\n",
       "<tr><td>2022-01-01 07:51:00</td><td>130,5,N,0026,5</td><td>+0094,5</td></tr>\n",
       "<tr><td>2022-01-01 08:15:00</td><td>120,5,N,0036,5</td><td>+0094,5</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------------------+--------------+-------+\n",
       "|               date|           wnd|    tmp|\n",
       "+-------------------+--------------+-------+\n",
       "|2022-01-01 00:00:00|200,1,N,0031,1|+0094,1|\n",
       "|2022-01-01 00:51:00|200,5,N,0031,5|+0094,5|\n",
       "|2022-01-01 01:51:00|200,5,N,0031,5|+0089,5|\n",
       "|2022-01-01 02:41:00|200,5,N,0036,5|+0094,5|\n",
       "|2022-01-01 02:51:00|220,5,N,0026,5|+0094,5|\n",
       "|2022-01-01 03:00:00|220,1,N,0026,1|+0094,1|\n",
       "|2022-01-01 03:16:00|200,5,N,0015,5|+0089,5|\n",
       "|2022-01-01 03:51:00|190,5,N,0026,5|+0089,5|\n",
       "|2022-01-01 04:17:00|180,5,N,0026,5|+0094,5|\n",
       "|2022-01-01 04:51:00|170,5,N,0036,5|+0094,5|\n",
       "|2022-01-01 04:59:00|999,9,9,9999,9|+9999,9|\n",
       "|2022-01-01 04:59:00|999,9,9,9999,9|+9999,9|\n",
       "|2022-01-01 05:49:00|180,5,N,0041,5|+0090,5|\n",
       "|2022-01-01 05:51:00|180,5,N,0036,5|+0094,5|\n",
       "|2022-01-01 06:00:00|180,1,N,0036,1|+0094,1|\n",
       "|2022-01-01 06:18:00|180,5,N,0026,5|+0089,5|\n",
       "|2022-01-01 06:51:00|160,5,N,0021,5|+0094,5|\n",
       "|2022-01-01 07:09:00|130,5,N,0015,5|+0094,5|\n",
       "|2022-01-01 07:51:00|130,5,N,0026,5|+0094,5|\n",
       "|2022-01-01 08:15:00|120,5,N,0036,5|+0094,5|\n",
       "+-------------------+--------------+-------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 mta weather\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Read the CSV files into Spark DataFrames\n",
    "weather_2022 = spark.read.csv(\"../data/landing/weather_link_2022.csv\", \\\n",
    "                              header=True)\n",
    "weather_2023 = spark.read.csv(\"../data/landing/weather_link_2023.csv\", \\\n",
    "                              header=True)\n",
    "print(weather_2022.count(), len(weather_2022.columns))\n",
    "print(weather_2023.count(), len(weather_2023.columns))\n",
    "\n",
    "# Define the columns to retain\n",
    "# the author argues to remove DEW as this is the dew temperature, which is \n",
    "# not commonly used in traditional weather forecasts on televisions or \n",
    "# our smartphones. In addition, we already have the tmp feature, which is the\n",
    "# normal measurement of temperature. If we retain dew, then we have 2 highly\n",
    "# positively correlated features inside the dataset, while the assumption is\n",
    "# to have independent columns which are correlated with the response variable\n",
    "columns_to_retain = ['DATE', 'WND', 'TMP']\n",
    "\n",
    "\n",
    "# Select the specified columns, and pass to convert to raw data layer\n",
    "# convert from landing to raw on weather datasets constrained by columns to \n",
    "# retain\n",
    "weather_2022 = landing_to_raw(weather_2022, columns_to_retain, 'date', True)\n",
    "weather_2023 = landing_to_raw(weather_2023, columns_to_retain, 'date', True)\n",
    "\n",
    "weather_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- wnd: string (nullable = true)\n",
      " |-- tmp: string (nullable = true)\n",
      "\n",
      "None\n",
      "Number of rows: 13344\n",
      "Number of columns: 3\n",
      "Column names: ['date', 'wnd', 'tmp']\n",
      "None\n",
      "Number of rows: 8475\n",
      "Number of columns: 3\n",
      "Column names: ['date', 'wnd', 'tmp']\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# schema to be handled later as this requires in depth analysis on splititng\n",
    "# Example: WND: 200,1,N,0031,1, the information that we want to extract is 200\n",
    "# therefore, still retain the original schema, and will deal with this later\n",
    "# in curated layer to design our own features\n",
    "print(weather_2022.printSchema())\n",
    "print(statistics(weather_2022))\n",
    "print(statistics(weather_2023))\n",
    "save_to_parquet(weather_2022, file_path=\"../data/raw/weather_2022\")\n",
    "save_to_parquet(weather_2023, file_path=\"../data/raw/weather_2023\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-------+\n",
      "|date               |wnd           |tmp    |\n",
      "+-------------------+--------------+-------+\n",
      "|2022-01-01 00:00:00|200,1,N,0031,1|+0094,1|\n",
      "|2022-01-01 00:51:00|200,5,N,0031,5|+0094,5|\n",
      "|2022-01-01 01:51:00|200,5,N,0031,5|+0089,5|\n",
      "|2022-01-01 02:41:00|200,5,N,0036,5|+0094,5|\n",
      "|2022-01-01 02:51:00|220,5,N,0026,5|+0094,5|\n",
      "|2022-01-01 03:00:00|220,1,N,0026,1|+0094,1|\n",
      "|2022-01-01 03:16:00|200,5,N,0015,5|+0089,5|\n",
      "|2022-01-01 03:51:00|190,5,N,0026,5|+0089,5|\n",
      "|2022-01-01 04:17:00|180,5,N,0026,5|+0094,5|\n",
      "|2022-01-01 04:51:00|170,5,N,0036,5|+0094,5|\n",
      "|2022-01-01 04:59:00|999,9,9,9999,9|+9999,9|\n",
      "|2022-01-01 04:59:00|999,9,9,9999,9|+9999,9|\n",
      "|2022-01-01 05:49:00|180,5,N,0041,5|+0090,5|\n",
      "|2022-01-01 05:51:00|180,5,N,0036,5|+0094,5|\n",
      "|2022-01-01 06:00:00|180,1,N,0036,1|+0094,1|\n",
      "|2022-01-01 06:18:00|180,5,N,0026,5|+0089,5|\n",
      "|2022-01-01 06:51:00|160,5,N,0021,5|+0094,5|\n",
      "|2022-01-01 07:09:00|130,5,N,0015,5|+0094,5|\n",
      "|2022-01-01 07:51:00|130,5,N,0026,5|+0094,5|\n",
      "|2022-01-01 08:15:00|120,5,N,0036,5|+0094,5|\n",
      "+-------------------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# quality check\n",
    "sample_2022 = spark.read.parquet(\"../data/raw/weather_2022\", header = True)\n",
    "sample_2022.show(20, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTA Subway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5553521 11\n",
      "+-------------------+-------+---------+----------------------------+\n",
      "|transit_timestamp  |borough|ridership|georeference                |\n",
      "+-------------------+-------+---------+----------------------------+\n",
      "|2023-08-12 11:00:00|M      |283      |POINT (-73.937965 40.851696)|\n",
      "|2023-08-12 13:00:00|BK     |148      |POINT (-73.92261 40.66472)  |\n",
      "|2022-10-08 21:00:00|M      |117      |POINT (-73.94748 40.7906)   |\n",
      "|2022-05-20 10:00:00|M      |356      |POINT (-73.94748 40.7906)   |\n",
      "|2022-03-14 08:00:00|M      |845      |POINT (-73.968376 40.799446)|\n",
      "|2022-05-03 22:00:00|M      |470      |POINT (-73.98163 40.730953) |\n",
      "|2023-01-28 18:00:00|M      |null     |POINT (-73.98163 40.730953) |\n",
      "|2023-07-07 08:00:00|M      |730      |POINT (-73.968376 40.799446)|\n",
      "|2022-02-28 20:00:00|M      |482      |POINT (-73.98163 40.730953) |\n",
      "|2022-03-03 00:00:00|M      |43       |POINT (-73.968376 40.799446)|\n",
      "|2022-09-08 13:00:00|M      |396      |POINT (-73.94748 40.7906)   |\n",
      "|2022-12-18 02:00:00|M      |16       |POINT (-73.968376 40.799446)|\n",
      "|2023-03-23 17:00:00|M      |680      |POINT (-73.968376 40.799446)|\n",
      "|2022-06-17 11:00:00|M      |443      |POINT (-73.968376 40.799446)|\n",
      "|2022-04-15 11:00:00|M      |361      |POINT (-73.94748 40.7906)   |\n",
      "|2022-06-18 00:00:00|M      |49       |POINT (-73.94748 40.7906)   |\n",
      "|2022-11-07 00:00:00|M      |30       |POINT (-73.968376 40.799446)|\n",
      "|2023-01-25 12:00:00|M      |384      |POINT (-73.94748 40.7906)   |\n",
      "|2022-08-27 21:00:00|M      |230      |POINT (-73.968376 40.799446)|\n",
      "|2022-06-14 00:00:00|M      |45       |POINT (-73.968376 40.799446)|\n",
      "+-------------------+-------+---------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply the same process on mta\n",
    "mta = spark.read.csv(\"../data/landing/mta_2023.csv\", header = True)\n",
    "print(mta.count(), len(mta.columns))\n",
    "\n",
    "\n",
    "# retain ['transit_timestamp', 'borough', 'ridership']\n",
    "# remove transfers as this is already included in ridership based on \n",
    "# transfers: those entering subway either bus-to-subway or out-of-network\n",
    "columns_to_retain = ['transit_timestamp', 'borough', 'ridership', \n",
    "                     'Georeference']\n",
    "\n",
    "# Convert string to timestamp\n",
    "mta = landing_to_raw(mta, columns_to_retain, 'transit_timestamp', False)\n",
    "# Show the DataFrame with converted timestamps\n",
    "mta.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transit_timestamp: timestamp (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- ridership: integer (nullable = true)\n",
      " |-- georeference: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "save_to_parquet(mta, file_path=\"../data/raw/mta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
